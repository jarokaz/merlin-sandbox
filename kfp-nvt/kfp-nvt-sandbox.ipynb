{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeedccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eef983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d599e823",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'jk-mlops-dev'\n",
    "REGION = 'us-central1'\n",
    "USER = 'test'\n",
    "\n",
    "STAGING_BUCKET = 'gs://jk-vertex-us-central1'\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/{}\".format(STAGING_BUCKET, 'pipeline_runs')\n",
    "VERTEX_SA = f'vertex-sa@{PROJECT_ID}.iam.gserviceaccount.com'\n",
    "\n",
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a93ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141d16ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component\n",
    "def preprocess(\n",
    "    # An input parameter of type string.\n",
    "    message: str,\n",
    "    # Use Output to get a metadata-rich handle to the output artifact\n",
    "    # of type `Dataset`.\n",
    "    output_dataset_one: Output[Dataset],\n",
    "    # A locally accessible filepath for another output artifact of type\n",
    "    # `Dataset`.\n",
    "    output_dataset_two_path: OutputPath(\"Dataset\"),\n",
    "    # A locally accessible filepath for an output parameter of type string.\n",
    "    output_parameter_path: OutputPath(str),\n",
    "):\n",
    "    \"\"\"'Mock' preprocessing step.\n",
    "    Writes out the passed in message to the output \"Dataset\"s and the output message.\n",
    "    \"\"\"\n",
    "    output_dataset_one.metadata[\"hello\"] = \"there\"\n",
    "    # Use OutputArtifact.path to access a local file path for writing.\n",
    "    # One can also use OutputArtifact.uri to access the actual URI file path.\n",
    "    with open(output_dataset_one.path, \"w\") as f:\n",
    "        f.write(message)\n",
    "\n",
    "    # OutputPath is used to just pass the local file path of the output artifact\n",
    "    # to the function.\n",
    "    with open(output_dataset_two_path, \"w\") as f:\n",
    "        f.write(message)\n",
    "\n",
    "    with open(output_parameter_path, \"w\") as f:\n",
    "        f.write(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17849c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",  # Use a different base image.\n",
    ")\n",
    "def train(\n",
    "    # An input parameter of type string.\n",
    "    message: str,\n",
    "    # Use InputPath to get a locally accessible path for the input artifact\n",
    "    # of type `Dataset`.\n",
    "    dataset_one_path: InputPath(\"Dataset\"),\n",
    "    # Use InputArtifact to get a metadata-rich handle to the input artifact\n",
    "    # of type `Dataset`.\n",
    "    dataset_two: Input[Dataset],\n",
    "    # Output artifact of type Model.\n",
    "    imported_dataset: Input[Dataset],\n",
    "    model: Output[Model],\n",
    "    # An input parameter of type int with a default value.\n",
    "    num_steps: int = 3,\n",
    "    # Use NamedTuple to return either artifacts or parameters.\n",
    "    # When returning artifacts like this, return the contents of\n",
    "    # the artifact. The assumption here is that this return value\n",
    "    # fits in memory.\n",
    ") -> NamedTuple(\n",
    "    \"Outputs\",\n",
    "    [\n",
    "        (\"output_message\", str),  # Return parameter.\n",
    "        (\"generic_artifact\", Artifact),  # Return generic Artifact.\n",
    "    ],\n",
    "):\n",
    "    \"\"\"'Mock' Training step.\n",
    "    Combines the contents of dataset_one and dataset_two into the\n",
    "    output Model.\n",
    "    Constructs a new output_message consisting of message repeated num_steps times.\n",
    "    \"\"\"\n",
    "\n",
    "    # Directly access the passed in GCS URI as a local file (uses GCSFuse).\n",
    "    with open(dataset_one_path, \"r\") as input_file:\n",
    "        dataset_one_contents = input_file.read()\n",
    "\n",
    "    # dataset_two is an Artifact handle. Use dataset_two.path to get a\n",
    "    # local file path (uses GCSFuse).\n",
    "    # Alternately, use dataset_two.uri to access the GCS URI directly.\n",
    "    with open(dataset_two.path, \"r\") as input_file:\n",
    "        dataset_two_contents = input_file.read()\n",
    "\n",
    "    with open(model.path, \"w\") as f:\n",
    "        f.write(\"My Model\")\n",
    "\n",
    "    with open(imported_dataset.path, \"r\") as f:\n",
    "        data = f.read()\n",
    "    print(\"Imported Dataset:\", data)\n",
    "\n",
    "    # Use model.get() to get a Model artifact, which has a .metadata dictionary\n",
    "    # to store arbitrary metadata for the output artifact. This metadata will be\n",
    "    # recorded in Managed Metadata and can be queried later. It will also show up\n",
    "    # in the UI.\n",
    "    model.metadata[\"accuracy\"] = 0.9\n",
    "    model.metadata[\"framework\"] = \"Tensorflow\"\n",
    "    model.metadata[\"time_to_train_in_seconds\"] = 257\n",
    "\n",
    "    artifact_contents = \"{}\\n{}\".format(dataset_one_contents, dataset_two_contents)\n",
    "    output_message = \" \".join([message for _ in range(num_steps)])\n",
    "    return (output_message, artifact_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35251ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component\n",
    "def read_artifact_input(\n",
    "    generic: Input[Artifact],\n",
    "):\n",
    "    with open(generic.path, \"r\") as input_file:\n",
    "        generic_contents = input_file.read()\n",
    "        print(f\"generic contents: {generic_contents}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33219aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    # Default pipeline root. You can override it when submitting the pipeline.\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    # A name for the pipeline. Use to determine the pipeline Context.\n",
    "    name=\"metadata-pipeline-v2\",\n",
    ")\n",
    "def pipeline(message: str):\n",
    "    importer = kfp.dsl.importer(\n",
    "        artifact_uri=\"gs://ml-pipeline-playground/shakespeare1.txt\",\n",
    "        artifact_class=Dataset,\n",
    "        reimport=False,\n",
    "    )\n",
    "    preprocess_task = preprocess(message=message)\n",
    "    train_task = train(\n",
    "        dataset_one=preprocess_task.outputs[\"output_dataset_one\"],\n",
    "        dataset_two=preprocess_task.outputs[\"output_dataset_two\"],\n",
    "        imported_dataset=importer.output,\n",
    "        message=preprocess_task.outputs[\"output_parameter\"],\n",
    "        num_steps=5,\n",
    "    )\n",
    "    read_task = read_artifact_input(  # noqa: F841\n",
    "        train_task.outputs[\"generic_artifact\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f328763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "package_path = \"hw_pipeline_job.json\"\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=package_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6707a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = 'test_pipeline_run'\n",
    "\n",
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=job_name,\n",
    "    template_path=package_path,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "pipeline_job.run(\n",
    "    service_account=VERTEX_SA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871ab2d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m78",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m78"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
