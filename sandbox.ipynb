{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c04fed73",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8efc0628",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-08 03:49:28.493363: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "# External Dependencies\n",
    "import numpy as np\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "import rmm\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "\n",
    "# NVTabular\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import (\n",
    "    Categorify,\n",
    "    Clip,\n",
    "    FillMissing,\n",
    "    Normalize,\n",
    ")\n",
    "from nvtabular.utils import _pynvml_mem_size, device_mem_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88af9414",
   "metadata": {},
   "source": [
    "## Configure GCP settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b96f958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'jk-mlops-dev'\n",
    "REGION = 'us-central1'\n",
    "\n",
    "STAGING_BUCKET = 'gs://jk-vertex-us-central1'\n",
    "\n",
    "VERTEX_SA = f'vertex-sa@{PROJECT}.iam.gserviceaccount.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155b880d",
   "metadata": {},
   "source": [
    "## Prepare a preprocessing container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b410d139",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGE = f'gcr.io/{PROJECT}/merlin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b65c7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_FOLDER = 'preprocess'\n",
    "if tf.io.gfile.exists(SCRIPT_FOLDER):\n",
    "    tf.io.gfile.rmtree(SCRIPT_FOLDER)\n",
    "tf.io.gfile.mkdir(SCRIPT_FOLDER)\n",
    "file_path = os.path.join(SCRIPT_FOLDER, 'preprocess.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e014ad",
   "metadata": {},
   "source": [
    "### Prepare a Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2257ab25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {SCRIPT_FOLDER}/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cu110\n",
    "\n",
    "WORKDIR /nvtabular\n",
    "\n",
    "RUN conda install -c nvidia -c rapidsai -c numba -c conda-forge pynvml dask-cuda nvtabular=0.5.3  cudatoolkit=11.0\n",
    "\n",
    "ENV LD_LIBRARY_PATH /usr/local/cuda/lib:/usr/local/cuda/lib64:/usr/local/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
    "ENV PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION python\n",
    "\n",
    "COPY preprocess.py ./\n",
    "#COPY entrypoint.sh ./\n",
    "#RUN chmod +x ./entrypoint.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5bd67b",
   "metadata": {},
   "source": [
    "### Prepare a preprocessing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "be6aaaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {file_path}\n",
    "\n",
    "\n",
    "# Copyright 2021 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import warnings\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import nvtabular as nvt\n",
    "import rmm\n",
    "\n",
    "from datetime import datetime\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "from nvtabular.ops import (\n",
    "    Categorify,\n",
    "    Clip,\n",
    "    FillMissing,\n",
    "    Normalize,\n",
    ")\n",
    "from nvtabular.utils import _pynvml_mem_size, device_mem_size\n",
    "\n",
    "\n",
    "BASE_DIR = '/tmp'\n",
    "DASK_CLUSTER_PROTOCOL = 'tcp'\n",
    "DASHBOARD_PORT = '8787'\n",
    "\n",
    "# Criteo columns\n",
    "CONTINUOUS_COLUMNS = [\"I\" + str(x) for x in range(1, 14)]\n",
    "CATEGORICAL_COLUMNS = [\"C\" + str(x) for x in range(1, 27)]\n",
    "LABEL_COLUMNS = [\"label\"]\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    \"\"\"Defines and parse commandline arguments.\"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--output_path\",\n",
    "        default=\"/tmp\",\n",
    "        type=str,\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--training_data\",\n",
    "        default=\"/tmp/training\",\n",
    "        type=str,\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--validation_data\",\n",
    "        default=\"/tmp/validation\",\n",
    "        type=str,\n",
    "    )\n",
    "        \n",
    "    parser.add_argument(\n",
    "        \"--gpus\",\n",
    "        default=\"0,1\",\n",
    "        type=str,\n",
    "    )\n",
    "        \n",
    "    parser.add_argument(\n",
    "        \"--device_limit_frac\",\n",
    "        default=0.7,\n",
    "        type=float,\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--device_pool_frac\",\n",
    "        default=0.8,\n",
    "        type=float,\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--part_mem_frac\",\n",
    "        default=0.1,\n",
    "        type=float,\n",
    "    )\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def create_dask_cuda_cluster(\n",
    "    gpus,\n",
    "    device_size,\n",
    "    device_limit_frac,\n",
    "    device_pool_frac,\n",
    "    dask_workdir,\n",
    "):\n",
    "    \n",
    "    # Initialize RMM pool on ALL workers\n",
    "    def _rmm_pool():\n",
    "        rmm.reinitialize(\n",
    "            pool_allocator=True,\n",
    "            initial_pool_size=(device_pool_size // 256) * 256,  # Use default size\n",
    "        )\n",
    "    \n",
    "    device_limit = int(device_limit_frac * device_size)\n",
    "    device_pool_size = int(device_pool_frac * device_size)\n",
    "    \n",
    "    # Check if any device memory is already occupied\n",
    "    for dev in gpus.split(\",\"):\n",
    "        fmem = _pynvml_mem_size(kind=\"free\", index=int(dev))\n",
    "        used = (device_size - fmem) / 1e9\n",
    "        if used > 1.0:\n",
    "            warnings.warn(f\"BEWARE - {used} GB is already occupied on device {int(dev)}!\")\n",
    "            \n",
    "    cluster = LocalCUDACluster(\n",
    "        protocol=DASK_CLUSTER_PROTOCOL,\n",
    "        n_workers=len(gpus.split(\",\")),\n",
    "        CUDA_VISIBLE_DEVICES=gpus,\n",
    "        device_memory_limit=device_limit,\n",
    "        local_directory=dask_workdir\n",
    "    )  \n",
    "    \n",
    "    client = Client(cluster)\n",
    "    client.run(_rmm_pool)\n",
    "    \n",
    "    return client\n",
    "\n",
    "\n",
    "def create_preprocessing_workflow(\n",
    "    client,\n",
    "    stats_path,\n",
    "    num_buckets=10000000\n",
    "):\n",
    "    \n",
    "    categorify_op = Categorify(out_path=stats_path, max_size=num_buckets)\n",
    "    cat_features = CATEGORICAL_COLUMNS >> categorify_op\n",
    "    cont_features = CONTINUOUS_COLUMNS >> FillMissing() >> Clip(min_value=0) >> Normalize()\n",
    "    features = cat_features + cont_features + LABEL_COLUMNS\n",
    "    workflow = nvt.Workflow(features, client=client)\n",
    "    \n",
    "    return workflow\n",
    "\n",
    "def create_datasets(\n",
    "    train_paths,\n",
    "    valid_paths,\n",
    "    part_mem_frac,\n",
    "    device_size,\n",
    "):\n",
    "    \n",
    "    dict_dtypes = {}\n",
    "\n",
    "    for col in CATEGORICAL_COLUMNS:\n",
    "        dict_dtypes[col] = np.int64\n",
    "\n",
    "    for col in CONTINUOUS_COLUMNS:\n",
    "        dict_dtypes[col] = np.float32\n",
    "\n",
    "    for col in LABEL_COLUMNS:\n",
    "        dict_dtypes[col] = np.float32\n",
    "        \n",
    "    part_size = int(part_mem_frac * device_size)\n",
    "    train_dataset = nvt.Dataset(train_paths, engine=\"parquet\", part_size=part_size)\n",
    "    valid_dataset = nvt.Dataset(valid_paths, engine=\"parquet\", part_size=part_size)\n",
    "    \n",
    "    return dict_dtypes, train_dataset, valid_dataset\n",
    "    \n",
    "\n",
    "def main():\n",
    "    args = get_args()\n",
    "    \n",
    "\n",
    "    dask_workdir = os.path.join(BASE_DIR, \"test_dask/workdir\")\n",
    "    stats_path = os.path.join(BASE_DIR, \"test_dask/stats\")\n",
    "\n",
    "    # Make sure we have a clean worker space for Dask\n",
    "    if os.path.isdir(dask_workdir):\n",
    "        shutil.rmtree(dask_workdir)\n",
    "    os.makedirs(dask_workdir)\n",
    "\n",
    "    # Make sure we have a clean stats space for Dask\n",
    "    if os.path.isdir(stats_path):\n",
    "        shutil.rmtree(stats_path)\n",
    "    os.mkdir(stats_path)\n",
    "    \n",
    "\n",
    "    fname = \"day_{}.parquet\"\n",
    "    train_paths = [\n",
    "        os.path.join(args.training_data, filename) for filename in os.listdir(args.training_data)]\n",
    "    valid_paths = [\n",
    "        os.path.join(args.validation_data, filename) for filename in os.listdir(args.validation_data)]\n",
    "    \n",
    "    logging.info(f\"Training data path: {train_paths}\")\n",
    "    logging.info(f\"Validation data path: {valid_paths}\")\n",
    "    \n",
    "    logging.info(\"Creating Dask-Cuda cluster\")\n",
    "    device_size = device_mem_size(kind=\"total\")\n",
    "    client = create_dask_cuda_cluster(\n",
    "        gpus=args.gpus,\n",
    "        device_size=device_size,\n",
    "        device_limit_frac=args.device_limit_frac,\n",
    "        device_pool_frac=args.device_pool_frac,\n",
    "        dask_workdir=dask_workdir,\n",
    "    )\n",
    "    logging.info(\"Cluster created\")\n",
    "    logging.info(str(client))\n",
    "    \n",
    "    logging.info(\"Creating workflow\")\n",
    "    workflow = create_preprocessing_workflow(\n",
    "        client=client,\n",
    "        stats_path=stats_path)\n",
    "    logging.info(\"Workflow created\")\n",
    "    \n",
    "    logging.info(\"Creating datasets\")\n",
    "    dict_dtypes, train_dataset, valid_dataset = create_datasets(\n",
    "        train_paths=train_paths,\n",
    "        valid_paths=train_paths,\n",
    "        part_mem_frac=args.part_mem_frac,\n",
    "        device_size=device_size,\n",
    "    )\n",
    "    logging.info(\"Datasets created\")\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    logging.info(f\"Starting fitting the preprocessing workflow on a training dataset. Datetime: {start_time}\")\n",
    "    workflow.fit(train_dataset)\n",
    "    end_time = datetime.now()\n",
    "    logging.info('Fitting completed. Datetime: {}, Elapsed time: {}'.format(end_time, end_time-start_time))\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    logging.info(f\"Starting  the preprocessing workflow on a training dataset. Datetime: {start_time}\")\n",
    "    workflow.transform(train_dataset).to_parquet(\n",
    "        output_path=f'{args.output_path}/train',\n",
    "        shuffle=nvt.io.Shuffle.PER_PARTITION,\n",
    "        dtypes=dict_dtypes,\n",
    "        cats=CATEGORICAL_COLUMNS,\n",
    "        conts=CONTINUOUS_COLUMNS,\n",
    "        labels=LABEL_COLUMNS,\n",
    "    )\n",
    "    end_time = datetime.now()\n",
    "    logging.info('Processing completed. Datetime: {}, Elapsed time: {}'.format(end_time, end_time-start_time))\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    logging.info(f\"Starting the preprocessing workflow on a validation datasets. Datetime: {start_time}\")\n",
    "    workflow.transform(valid_dataset).to_parquet(\n",
    "        output_path=f'{args.output_path}/valid',\n",
    "        shuffle=nvt.io.Shuffle.PER_PARTITION,\n",
    "        dtypes=dict_dtypes,\n",
    "        cats=CATEGORICAL_COLUMNS,\n",
    "        conts=CONTINUOUS_COLUMNS,\n",
    "        labels=LABEL_COLUMNS,\n",
    "    )\n",
    "    end_time = datetime.now()\n",
    "    logging.info('Processing completed. Datetime: {}, Elapsed time: {}'.format(end_time, end_time-start_time))\n",
    "    \n",
    "    logging.info(f\"Saving workflow to {args.output_path}\")\n",
    "    workflow.save(os.path.join(args.output_path, \"workflow\"))\n",
    "    logging.info(\"Workflow saved\")\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe9b214",
   "metadata": {},
   "source": [
    "### Build a container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7020b1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  10.75kB\n",
      "Step 1/6 : FROM gcr.io/deeplearning-platform-release/base-cu110\n",
      " ---> a88534d17a8b\n",
      "Step 2/6 : WORKDIR /nvtabular\n",
      " ---> Using cache\n",
      " ---> 29b40fc1e76a\n",
      "Step 3/6 : RUN conda install -c nvidia -c rapidsai -c numba -c conda-forge pynvml dask-cuda nvtabular=0.5.3  cudatoolkit=11.0\n",
      " ---> Using cache\n",
      " ---> d12c90589672\n",
      "Step 4/6 : ENV LD_LIBRARY_PATH /usr/local/cuda/lib:/usr/local/cuda/lib64:/usr/local/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      " ---> Using cache\n",
      " ---> d67437c6dbc9\n",
      "Step 5/6 : ENV PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION python\n",
      " ---> Using cache\n",
      " ---> 323e76b8ce44\n",
      "Step 6/6 : COPY preprocess.py ./\n",
      " ---> Using cache\n",
      " ---> 0b5fb7d6f6b7\n",
      "Successfully built 0b5fb7d6f6b7\n",
      "Successfully tagged gcr.io/jk-mlops-dev/merlin-preprocess:latest\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/2807180729060950016 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/2807180729060950016 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "TRAIN_IMAGE = f'gcr.io/{PROJECT}/merlin-preprocess'\n",
    "\n",
    "! docker build -t {TRAIN_IMAGE} {SCRIPT_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "af6a254c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "The push refers to repository [gcr.io/jk-mlops-dev/merlin-preprocess]\n",
      "\n",
      "\u001b[1Bc9099694: Preparing \n",
      "\u001b[1Bb29a95c2: Preparing \n",
      "\u001b[1Baca5bb00: Preparing \n",
      "\u001b[1Bc6abb710: Preparing \n",
      "\u001b[1B119bf2cd: Preparing \n",
      "\u001b[1Bbdf9b557: Preparing \n",
      "\u001b[1Bdbc2b748: Preparing \n",
      "\u001b[1Bb8f29c2e: Preparing \n",
      "\u001b[1B7b2f7486: Preparing \n",
      "\u001b[1B97a3e6e4: Preparing \n",
      "\u001b[1Ba5e8117f: Preparing \n",
      "\u001b[1B8124ed57: Preparing \n",
      "\u001b[1B4704bb3d: Preparing \n",
      "\u001b[1B6ef24b4b: Preparing \n",
      "\u001b[1B113f67c8: Preparing \n",
      "\u001b[1B857a1d48: Preparing \n",
      "\u001b[1B97864c52: Preparing \n",
      "\u001b[1Bbaac3e32: Preparing \n",
      "\u001b[1Ba1af4c10: Preparing \n",
      "\u001b[1Ba468ca49: Preparing \n",
      "\u001b[1B205798d1: Preparing \n",
      "\u001b[13B7a3e6e4: Waiting g \n",
      "\u001b[1B55c89c2a: Preparing \n",
      "\u001b[14B5e8117f: Waiting g \n",
      "\u001b[14B124ed57: Waiting g \n",
      "\u001b[1B9ca3db46: Preparing \n",
      "\u001b[1B1a1930ab: Preparing \n",
      "\u001b[22Bbc2b748: Waiting g \n",
      "\u001b[29B9099694: Pushed lready exists 7kB\u001b[29A\u001b[2K\u001b[27A\u001b[2K\u001b[25A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[18A\u001b[2K\u001b[16A\u001b[2K\u001b[14A\u001b[2K\u001b[11A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[29A\u001b[2Klatest: digest: sha256:51798be2a915680030ca34675e0bd843828be0d9bcb22d00ba668a970922f8c7 size: 6407\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/2807180729060950016 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "! docker push {TRAIN_IMAGE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6d1f1f",
   "metadata": {},
   "source": [
    "## Submit Vertex job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e82f36d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "382efcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'machine_spec': {'machine_type': 'n1-standard-8', 'accelerator_type': 'NVIDIA_TESLA_T4', 'accelerator_count': 2}, 'replica_count': 1, 'container_spec': {'image_uri': 'gcr.io/jk-mlops-dev/merlin-preprocess', 'command': ['python', 'preprocess.py'], 'args': ['--training_data=/gcs/jk-vertex-us-central1/criteo-small-train', '--validation_data=/gcs/jk-vertex-us-central1/criteo-small-valid', '--output_path=/gcs/jk-vertex-us-central1/merlin-testing/MERLIN_CONTAINER_TEST_20210808_232940']}}]\n"
     ]
    }
   ],
   "source": [
    "job_name = 'MERLIN_CONTAINER_TEST_{}'.format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "base_output_dir = '{}/jobs/{}/test.txt'.format(STAGING_BUCKET, job_name)\n",
    "\n",
    "training_data = '/gcs/jk-vertex-us-central1/criteo-small-train'\n",
    "validation_data = '/gcs/jk-vertex-us-central1/criteo-small-valid'\n",
    "output_path = f'/gcs/jk-vertex-us-central1/merlin-testing/{job_name}'\n",
    "\n",
    "worker_pool_specs =  [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-8\",\n",
    "            \"accelerator_type\": \"NVIDIA_TESLA_T4\",\n",
    "            \"accelerator_count\": 2,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_IMAGE,\n",
    "            \"command\": [\"python\", \"preprocess.py\",],\n",
    "            \"args\": [             \n",
    "                '--training_data=' + training_data, \n",
    "                '--validation_data=' + validation_data,\n",
    "                '--output_path=' + output_path,\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "print(worker_pool_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9613d4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating CustomJob\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob created. Resource name: projects/895222332033/locations/us-central1/customJobs/643201108109426688\n",
      "INFO:google.cloud.aiplatform.jobs:To use this CustomJob in another session:\n",
      "INFO:google.cloud.aiplatform.jobs:custom_job = aiplatform.CustomJob.get('projects/895222332033/locations/us-central1/customJobs/643201108109426688')\n",
      "INFO:google.cloud.aiplatform.jobs:View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/643201108109426688?project=895222332033\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/643201108109426688 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/643201108109426688 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/643201108109426688 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/643201108109426688 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/2807180729060950016 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/643201108109426688 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/643201108109426688 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/2807180729060950016 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/643201108109426688 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/2807180729060950016 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "job = vertex_ai.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=base_output_dir\n",
    ")\n",
    "\n",
    "job.run(sync=False, \n",
    "        service_account=VERTEX_SA,\n",
    "#        tensorboard=TENSORBOARD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36687f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m75"
  },
  "kernelspec": {
   "display_name": "nvt-11-2",
   "language": "python",
   "name": "nvt-11-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
