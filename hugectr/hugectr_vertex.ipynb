{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# HugeCTR training on Vertex"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import pprint\n",
    "import sys\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "from google.cloud import aiplatform"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GCP Settings\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "PROJECT = 'jk-mlops-dev'\n",
    "REGION = 'us-central1'\n",
    "API_ENDPOINT = f'{REGION}-aiplatform.googleapis.com'\n",
    "GCS_BUCKET = f'gs://jk-staging-{REGION}'\n",
    "\n",
    "VERTEX_SA = f'training-sa@{PROJECT}.iam.gserviceaccount.com'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create GCS bucket\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "objects = !gsutil ls {GCS_BUCKET}\n",
    "if objects:\n",
    "    if 'BucketNotFoundException' in objects[0]:\n",
    "        print('Creating a new bucket')\n",
    "        !gsutil mb -l {REGION} {GCS_BUCKET}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare a training container"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "SCRIPT_FOLDER = 'train'\n",
    "if not os.path.isdir(SCRIPT_FOLDER):\n",
    "   os.mkdir(SCRIPT_FOLDER)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a training script"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "%%writefile  {SCRIPT_FOLDER}/train.py\n",
    "\n",
    "# Copyright (c) 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "# Standard Libraries\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "def train(input_train, input_val, max_iter,\n",
    "                batchsize, snapshot, num_gpus, eval_interval,\n",
    "                dense_model_file, sparse_model_files):\n",
    "\n",
    "    logging.info(f\"GPU Devices: {num_gpus}\")\n",
    "\n",
    "    # Configure and define the HugeCTR model\n",
    "    solver = hugectr.solver_parser_helper(num_epochs = 0,\n",
    "                                        max_iter = max_iter,\n",
    "                                        max_eval_batches = 100,\n",
    "                                        batchsize_eval = batchsize,\n",
    "                                        batchsize = batchsize,\n",
    "                                        model_file = dense_model_file,\n",
    "                                        embedding_files = sparse_model_files,\n",
    "                                        display = 200,\n",
    "                                        eval_interval = eval_interval,\n",
    "                                        i64_input_key = True,\n",
    "                                        use_mixed_precision = False,\n",
    "                                        repeat_dataset = True,\n",
    "                                        snapshot = snapshot,\n",
    "                                        vvgpu = [num_gpus],\n",
    "                                        use_cuda_graph = False\n",
    "                                        )\n",
    "\n",
    "    optimizer = hugectr.optimizer.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam,\n",
    "                                        use_mixed_precision = False)\n",
    "    model = hugectr.Model(solver, optimizer)\n",
    "\n",
    "    # The slot_size_array are the cardinalities of each categorical feature after NVTabular preprocessing\n",
    "    model.add(hugectr.Input(data_reader_type = hugectr.DataReaderType_t.Parquet,\n",
    "                                source = input_train,\n",
    "                                eval_source = input_val,\n",
    "                                check_type = hugectr.Check_t.Non,\n",
    "                                label_dim = 1, label_name = \"label\",\n",
    "                                dense_dim = 13, dense_name = \"dense\",\n",
    "                                slot_size_array = [18576837, 29428, 15128, 7296, 19902, 4, 6466, 1311, 62, 11700067, 622921, 219557, 11, 2209, 9780, 71, 4, 964, 15, 22022124, 4384510, 15960286, 290588, 10830, 96, 35],\n",
    "                                data_reader_sparse_param_array =\n",
    "                                [hugectr.DataReaderSparseParam(hugectr.DataReaderSparse_t.Distributed, 30, 1, 26)],\n",
    "                                sparse_names = [\"data1\"]))\n",
    "\n",
    "    # Sparse Embedding Layer\n",
    "    model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash,\n",
    "                                max_vocabulary_size_per_gpu = 88656602,\n",
    "                                embedding_vec_size = 16,\n",
    "                                combiner = 0,\n",
    "                                sparse_embedding_name = \"sparse_embedding1\",\n",
    "                                bottom_name = \"data1\"))\n",
    "    model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                                bottom_names = [\"sparse_embedding1\"],\n",
    "                                top_names = [\"reshape1\"],\n",
    "                                leading_dim=416))\n",
    "\n",
    "    # Concatenate sparse embedding and dense input\n",
    "    model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                                bottom_names = [\"reshape1\", \"dense\"], top_names = [\"concat1\"]))\n",
    "    model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Slice,\n",
    "                                bottom_names = [\"concat1\"],\n",
    "                                top_names = [\"slice11\", \"slice12\"],\n",
    "                                ranges=[(0,429),(0,429)]))\n",
    "\n",
    "    model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.MultiCross,\n",
    "                                bottom_names = [\"slice11\"],\n",
    "                                top_names = [\"multicross1\"],\n",
    "                                num_layers=6))\n",
    "\n",
    "    model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                                bottom_names = [\"slice12\"],\n",
    "                                top_names = [\"fc1\"],\n",
    "                                num_output=1024))\n",
    "    model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                                bottom_names = [\"fc1\"],\n",
    "                                top_names = [\"relu1\"]))\n",
    "    model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                                bottom_names = [\"relu1\"],\n",
    "                                top_names = [\"dropout1\"],\n",
    "                                dropout_rate=0.5))\n",
    "    model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                                bottom_names = [\"dropout1\"],\n",
    "                                top_names = [\"fc2\"],\n",
    "                                num_output=1024))\n",
    "    model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                                bottom_names = [\"fc2\"],\n",
    "                                top_names = [\"relu2\"]))\n",
    "    model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                                bottom_names = [\"relu2\"],\n",
    "                                top_names = [\"dropout2\"],\n",
    "                                dropout_rate=0.5))\n",
    "\n",
    "    model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                                bottom_names = [\"dropout2\", \"multicross1\"],\n",
    "                                top_names = [\"concat2\"]))\n",
    "    model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                                bottom_names = [\"concat2\"],\n",
    "                                top_names = [\"fc3\"],\n",
    "                                num_output=1))\n",
    "    model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                                bottom_names = [\"fc3\", \"label\"],\n",
    "                                top_names = [\"loss\"]))\n",
    "    model.compile()\n",
    "    model.summary()\n",
    "    model.fit()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-t',\n",
    "                        '--input_train',\n",
    "                        type=str,\n",
    "                        required=False,\n",
    "                        default='/data/output/test_dask/output/train/_file_list.txt',\n",
    "                        help='Path to training data _file_list.txt')\n",
    "\n",
    "    parser.add_argument('-v',\n",
    "                        '--input_val',\n",
    "                        type=str,\n",
    "                        required=False,\n",
    "                        default='/data/output/test_dask/output/valid/_file_list.txt',\n",
    "                        help='Path to validation data _file_list.txt')\n",
    "\n",
    "    parser.add_argument('-i',\n",
    "                        '--max_iter',\n",
    "                        type=int,\n",
    "                        required=False,\n",
    "                        default=20000,\n",
    "                        help='Number of training iterations')\n",
    "\n",
    "    parser.add_argument('-b',\n",
    "                        '--batchsize',\n",
    "                        type=int,\n",
    "                        required=False,\n",
    "                        default=2048,\n",
    "                        help='Batch size')\n",
    "\n",
    "    parser.add_argument('-s',\n",
    "                        '--snapshot',\n",
    "                        type=int,\n",
    "                        required=False,\n",
    "                        default=10000,\n",
    "                        help='Saves a model snapshot after given number of iterations')\n",
    "\n",
    "    parser.add_argument('-g',\n",
    "                        '--num_gpus',\n",
    "                        nargs='+',\n",
    "                        type=int,\n",
    "                        required=False,\n",
    "                        default=[0,1],\n",
    "                        help='GPU devices to use for Preprocessing')\n",
    "\n",
    "    parser.add_argument('-r',\n",
    "                        '--eval_interval',\n",
    "                        type=int,\n",
    "                        required=False,\n",
    "                        default=1000,\n",
    "                        help='Run evaluation after given number of iterations')\n",
    "\n",
    "    parser.add_argument('-d',\n",
    "                        '--dense_model_file',\n",
    "                        type=str,\n",
    "                        required=False,\n",
    "                        default=None,\n",
    "                        help='Path to an existing dense model. If provided, resumes training from here. Eg. ./_dense_19500.model ')\n",
    "\n",
    "    parser.add_argument('-m',\n",
    "                        '--sparse_model_files',\n",
    "                        type=str,\n",
    "                        nargs='+',\n",
    "                        required=False,\n",
    "                        default=None,\n",
    "                        help='Paths to an existing sparse snapshots. If provided, resumes training from here. Eg. --sparse_model_files ./model-snapshot/0_sparse_19500.model ./model-snapshot/0_sparse_19500.model')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO, datefmt='%d-%m-%y %H:%M:%S')\n",
    "\n",
    "    logging.info(f\"Args: {args}\")\n",
    "\n",
    "    # Both the dense and sparse model files should be provided if either one is provided\n",
    "    if args.dense_model_file and args.sparse_model_files:\n",
    "        logging.info(\"Training from previously saved model...\")\n",
    "        logging.info(f\"Dense model file: {args.dense_model_file}\")\n",
    "        logging.info(f\"Sparse model file: {args.sparse_model_files}\")\n",
    "        dense_model_file = args.dense_model_file\n",
    "        sparse_model_files = args.sparse_model_files\n",
    "    elif (args.dense_model_file and args.sparse_model_files is None) or \\\n",
    "                            (args.sparse_model_files and args.dense_model_file is None):\n",
    "        parser.error(\"--dense_model_file and --sparse_model_files both need to be provided together.\")\n",
    "    else:\n",
    "        logging.info(\"No previous checkpoint/model provided. Training from scratch. \")\n",
    "        dense_model_file = \"\"\n",
    "        sparse_model_files = []\n",
    "\n",
    "    train(input_train=args.input_train,\n",
    "            input_val=args.input_val,\n",
    "            max_iter=args.max_iter,\n",
    "            batchsize=args.batchsize,\n",
    "            snapshot=args.snapshot,\n",
    "            eval_interval=args.eval_interval,\n",
    "            num_gpus=args.num_gpus,\n",
    "            dense_model_file=dense_model_file,\n",
    "            sparse_model_files=sparse_model_files\n",
    "            )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting train/train.py\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a Dockerfile"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "%%writefile {SCRIPT_FOLDER}/Dockerfile\n",
    "FROM gcr.io/deeplearning-platform-release/base-cu110\n",
    "\n",
    "WORKDIR /src\n",
    "\n",
    "RUN conda install -c nvidia -c rapidsai -c numba -c conda-forge pynvml dask-cuda nvtabular=0.5.3  cudatoolkit=11.0\n",
    "ENV LD_LIBRARY_PATH /usr/local/cuda/lib:/usr/local/cuda/lib64:/usr/local/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
    "\n",
    "COPY train.py ./"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting train/Dockerfile\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "%%writefile {SCRIPT_FOLDER}/Dockerfile\n",
    "FROM nvcr.io/nvidia/merlin/merlin-training:0.5.3\n",
    "\n",
    "WORKDIR /src\n",
    "\n",
    "COPY train.py ./"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting train/Dockerfile\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build a container image"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "TRAIN_IMAGE = f'gcr.io/{PROJECT}/hugectr_train_test'\n",
    "\n",
    "!docker build -t {TRAIN_IMAGE} {SCRIPT_FOLDER}"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sending build context to Docker daemon  27.14kB\n",
      "Step 1/3 : FROM nvcr.io/nvidia/merlin/merlin-training:0.5.3\n",
      " ---> 332a8cffc9df\n",
      "Step 2/3 : WORKDIR /src\n",
      " ---> Running in 1581352bef97\n",
      "Removing intermediate container 1581352bef97\n",
      " ---> f981810ed293\n",
      "Step 3/3 : COPY train.py ./\n",
      " ---> 19725f1f23f2\n",
      "Successfully built 19725f1f23f2\n",
      "Successfully tagged gcr.io/jk-mlops-dev/hugectr_train_test:latest\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare and submit a job"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "aiplatform.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=GCS_BUCKET\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prepare worker pool specification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "TRAIN_IMAGE = 'gcr.io/jk-mlops-dev/merlin-train'\n",
    "\n",
    "worker_pool_specs =  [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"a2-highgpu-2g\",\n",
    "            \"accelerator_type\": \"NVIDIA_TESLA_A100\",\n",
    "            \"accelerator_count\": 1,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_IMAGE,\n",
    "            \"command\": [\"python\", \"train.py\"],\n",
    "            \"args\": [\n",
    "                '--input_train=' + , \n",
    "                '--per_replica_batch_size=128',\n",
    "                '--training_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}',\n",
    "                '--validation_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_VALID_SPLIT_NAME}',\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_data = '/gcs/jk-vertex-us-central1/criteo-processed/train'\n",
    "valid_data = '/gcs/jk-vertex-us-central1/criteo-processed/valid'\n",
    "\n",
    "worker_pool_specs =  [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"a2-highgpu-2g\",\n",
    "            \"accelerator_type\": \"NVIDIA_TESLA_A100\",\n",
    "            \"accelerator_count\": 2,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_IMAGE,\n",
    "            \"command\": [\"python\", \"train.py\"],\n",
    "            \"args\": [\n",
    "                '--input_train=' + train_data, \n",
    "                '--input_val=' + valid_data,\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "pp = pprint.PrettyPrinter()\n",
    "print(pp.pformat(worker_pool_specs))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "job_name = 'HUGECTR_{}'.format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "base_output_dir = '{}/jobs/{}'.format(GCS_BUCKET, job_name)\n",
    "\n",
    "job = aiplatform.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=base_output_dir\n",
    ")\n",
    "\n",
    "job.run(sync=False, \n",
    "#        service_account=VERTEX_SA,\n",
    "#        tensorboard=TENSORBOARD\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m76",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m76"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}