{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a66ecfa4",
   "metadata": {},
   "source": [
    "# HugeCTR training on Vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5761032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import sys\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c49f14",
   "metadata": {},
   "source": [
    "### GCP Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17da40da",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'jk-mlops-dev'\n",
    "REGION = 'us-central1'\n",
    "API_ENDPOINT = f'{REGION}-aiplatform.googleapis.com'\n",
    "GCS_BUCKET = f'gs://jk-staging-{REGION}'\n",
    "\n",
    "VERTEX_SA = f'training-sa@{PROJECT}.iam.gserviceaccount.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3301e1de",
   "metadata": {},
   "source": [
    "### Create GCS bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7524189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = !gsutil ls {GCS_BUCKET}\n",
    "if objects:\n",
    "    if 'BucketNotFoundException' in objects[0]:\n",
    "        print('Creating a new bucket')\n",
    "        !gsutil mb -l {REGION} {GCS_BUCKET}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddf4747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bcc38b5",
   "metadata": {},
   "source": [
    "## Prepare a training container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b1e26db",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_FOLDER = 'train'\n",
    "if not os.path.isdir(SCRIPT_FOLDER):\n",
    "   os.mkdir(SCRIPT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb726607",
   "metadata": {},
   "source": [
    "### Create a training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "841b9968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile  {SCRIPT_FOLDER}/train.py\n",
    "\n",
    "# Copyright (c) 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "# Standard Libraries\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "def train(input_train, input_val, max_iter,\n",
    "                batchsize, snapshot, num_gpus, eval_interval,\n",
    "                dense_model_file, sparse_model_files):\n",
    "\n",
    "    logging.info(f\"GPU Devices: {num_gpus}\")\n",
    "\n",
    "    # Configure and define the HugeCTR model\n",
    "    solver = hugectr.solver_parser_helper(num_epochs = 0,\n",
    "                                        max_iter = max_iter,\n",
    "                                        max_eval_batches = 100,\n",
    "                                        batchsize_eval = batchsize,\n",
    "                                        batchsize = batchsize,\n",
    "                                        model_file = dense_model_file,\n",
    "                                        embedding_files = sparse_model_files,\n",
    "                                        display = 200,\n",
    "                                        eval_interval = eval_interval,\n",
    "                                        i64_input_key = True,\n",
    "                                        use_mixed_precision = False,\n",
    "                                        repeat_dataset = True,\n",
    "                                        snapshot = snapshot,\n",
    "                                        vvgpu = [num_gpus],\n",
    "                                        use_cuda_graph = False\n",
    "                                        )\n",
    "\n",
    "    optimizer = hugectr.optimizer.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam,\n",
    "                                        use_mixed_precision = False)\n",
    "    model = hugectr.Model(solver, optimizer)\n",
    "\n",
    "    # The slot_size_array are the cardinalities of each categorical feature after NVTabular preprocessing\n",
    "    model.add(hugectr.Input(data_reader_type = hugectr.DataReaderType_t.Parquet,\n",
    "                                source = input_train,\n",
    "                                eval_source = input_val,\n",
    "                                check_type = hugectr.Check_t.Non,\n",
    "                                label_dim = 1, label_name = \"label\",\n",
    "                                dense_dim = 13, dense_name = \"dense\",\n",
    "                                slot_size_array = [18576837, 29428, 15128, 7296, 19902, 4, 6466, 1311, 62, 11700067, 622921, 219557, 11, 2209, 9780, 71, 4, 964, 15, 22022124, 4384510, 15960286, 290588, 10830, 96, 35],\n",
    "                                data_reader_sparse_param_array =\n",
    "                                [hugectr.DataReaderSparseParam(hugectr.DataReaderSparse_t.Distributed, 30, 1, 26)],\n",
    "                                sparse_names = [\"data1\"]))\n",
    "\n",
    "    # Sparse Embedding Layer\n",
    "    model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash,\n",
    "                                max_vocabulary_size_per_gpu = 88656602,\n",
    "                                embedding_vec_size = 16,\n",
    "                                combiner = 0,\n",
    "                                sparse_embedding_name = \"sparse_embedding1\",\n",
    "                                bottom_name = \"data1\"))\n",
    "    model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                                bottom_names = [\"sparse_embedding1\"],\n",
    "                                top_names = [\"reshape1\"],\n",
    "                                leading_dim=416))\n",
    "\n",
    "    # Concatenate sparse embedding and dense input\n",
    "    model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                                bottom_names = [\"reshape1\", \"dense\"], top_names = [\"concat1\"]))\n",
    "    model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Slice,\n",
    "                                bottom_names = [\"concat1\"],\n",
    "                                top_names = [\"slice11\", \"slice12\"],\n",
    "                                ranges=[(0,429),(0,429)]))\n",
    "\n",
    "    model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.MultiCross,\n",
    "                                bottom_names = [\"slice11\"],\n",
    "                                top_names = [\"multicross1\"],\n",
    "                                num_layers=6))\n",
    "\n",
    "    model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                                bottom_names = [\"slice12\"],\n",
    "                                top_names = [\"fc1\"],\n",
    "                                num_output=1024))\n",
    "    model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                                bottom_names = [\"fc1\"],\n",
    "                                top_names = [\"relu1\"]))\n",
    "    model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                                bottom_names = [\"relu1\"],\n",
    "                                top_names = [\"dropout1\"],\n",
    "                                dropout_rate=0.5))\n",
    "    model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                                bottom_names = [\"dropout1\"],\n",
    "                                top_names = [\"fc2\"],\n",
    "                                num_output=1024))\n",
    "    model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                                bottom_names = [\"fc2\"],\n",
    "                                top_names = [\"relu2\"]))\n",
    "    model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                                bottom_names = [\"relu2\"],\n",
    "                                top_names = [\"dropout2\"],\n",
    "                                dropout_rate=0.5))\n",
    "\n",
    "    model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                                bottom_names = [\"dropout2\", \"multicross1\"],\n",
    "                                top_names = [\"concat2\"]))\n",
    "    model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                                bottom_names = [\"concat2\"],\n",
    "                                top_names = [\"fc3\"],\n",
    "                                num_output=1))\n",
    "    model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                                bottom_names = [\"fc3\", \"label\"],\n",
    "                                top_names = [\"loss\"]))\n",
    "    model.compile()\n",
    "    model.summary()\n",
    "    model.fit()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-t',\n",
    "                        '--input_train',\n",
    "                        type=str,\n",
    "                        required=False,\n",
    "                        default='/data/output/test_dask/output/train/_file_list.txt',\n",
    "                        help='Path to training data _file_list.txt')\n",
    "\n",
    "    parser.add_argument('-v',\n",
    "                        '--input_val',\n",
    "                        type=str,\n",
    "                        required=False,\n",
    "                        default='/data/output/test_dask/output/valid/_file_list.txt',\n",
    "                        help='Path to validation data _file_list.txt')\n",
    "\n",
    "    parser.add_argument('-i',\n",
    "                        '--max_iter',\n",
    "                        type=int,\n",
    "                        required=False,\n",
    "                        default=20000,\n",
    "                        help='Number of training iterations')\n",
    "\n",
    "    parser.add_argument('-b',\n",
    "                        '--batchsize',\n",
    "                        type=int,\n",
    "                        required=False,\n",
    "                        default=2048,\n",
    "                        help='Batch size')\n",
    "\n",
    "    parser.add_argument('-s',\n",
    "                        '--snapshot',\n",
    "                        type=int,\n",
    "                        required=False,\n",
    "                        default=10000,\n",
    "                        help='Saves a model snapshot after given number of iterations')\n",
    "\n",
    "    parser.add_argument('-g',\n",
    "                        '--num_gpus',\n",
    "                        nargs='+',\n",
    "                        type=int,\n",
    "                        required=False,\n",
    "                        default=[0,1],\n",
    "                        help='GPU devices to use for Preprocessing')\n",
    "\n",
    "    parser.add_argument('-r',\n",
    "                        '--eval_interval',\n",
    "                        type=int,\n",
    "                        required=False,\n",
    "                        default=1000,\n",
    "                        help='Run evaluation after given number of iterations')\n",
    "\n",
    "    parser.add_argument('-d',\n",
    "                        '--dense_model_file',\n",
    "                        type=str,\n",
    "                        required=False,\n",
    "                        default=None,\n",
    "                        help='Path to an existing dense model. If provided, resumes training from here. Eg. ./_dense_19500.model ')\n",
    "\n",
    "    parser.add_argument('-m',\n",
    "                        '--sparse_model_files',\n",
    "                        type=str,\n",
    "                        nargs='+',\n",
    "                        required=False,\n",
    "                        default=None,\n",
    "                        help='Paths to an existing sparse snapshots. If provided, resumes training from here. Eg. --sparse_model_files ./model-snapshot/0_sparse_19500.model ./model-snapshot/0_sparse_19500.model')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO, datefmt='%d-%m-%y %H:%M:%S')\n",
    "\n",
    "    logging.info(f\"Args: {args}\")\n",
    "\n",
    "    # Both the dense and sparse model files should be provided if either one is provided\n",
    "    if args.dense_model_file and args.sparse_model_files:\n",
    "        logging.info(\"Training from previously saved model...\")\n",
    "        logging.info(f\"Dense model file: {args.dense_model_file}\")\n",
    "        logging.info(f\"Sparse model file: {args.sparse_model_files}\")\n",
    "        dense_model_file = args.dense_model_file\n",
    "        sparse_model_files = args.sparse_model_files\n",
    "    elif (args.dense_model_file and args.sparse_model_files is None) or \\\n",
    "                            (args.sparse_model_files and args.dense_model_file is None):\n",
    "        parser.error(\"--dense_model_file and --sparse_model_files both need to be provided together.\")\n",
    "    else:\n",
    "        logging.info(\"No previous checkpoint/model provided. Training from scratch. \")\n",
    "        dense_model_file = \"\"\n",
    "        sparse_model_files = []\n",
    "\n",
    "    train(input_train=args.input_train,\n",
    "            input_val=args.input_val,\n",
    "            max_iter=args.max_iter,\n",
    "            batchsize=args.batchsize,\n",
    "            snapshot=args.snapshot,\n",
    "            eval_interval=args.eval_interval,\n",
    "            num_gpus=args.num_gpus,\n",
    "            dense_model_file=dense_model_file,\n",
    "            sparse_model_files=sparse_model_files\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ce35b4",
   "metadata": {},
   "source": [
    "### Create a Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a827278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {SCRIPT_FOLDER}/Dockerfile\n",
    "FROM gcr.io/deeplearning-platform-release/base-cu110\n",
    "\n",
    "WORKDIR /src\n",
    "\n",
    "RUN conda install -c nvidia -c rapidsai -c numba -c conda-forge pynvml dask-cuda nvtabular=0.5.3  cudatoolkit=11.0\n",
    "ENV LD_LIBRARY_PATH /usr/local/cuda/lib:/usr/local/cuda/lib64:/usr/local/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
    "\n",
    "COPY train.py ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba620a2e",
   "metadata": {},
   "source": [
    "### Build a container image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51c2983d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  28.16kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cu110\n",
      " ---> a88534d17a8b\n",
      "Step 2/5 : WORKDIR /src\n",
      " ---> Running in 4c96bd5d9b58\n",
      "Removing intermediate container 4c96bd5d9b58\n",
      " ---> 42be7ad9830d\n",
      "Step 3/5 : RUN conda install -c nvidia -c rapidsai -c numba -c conda-forge pynvml dask-cuda nvtabular=0.5.3  cudatoolkit=11.0\n",
      " ---> Running in 53c998e17774\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: ...working... failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - cudatoolkit=11.0\n",
      "    - dask-cuda\n",
      "    - nvtabular=0.5.3\n",
      "    - pynvml\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    arrow-cpp-1.0.1            |py37h20531c0_43_cuda        21.0 MB  conda-forge\n",
      "    arrow-cpp-proc-3.0.0       |             cuda          24 KB  conda-forge\n",
      "    bokeh-2.3.3                |   py37h89c1867_0         8.2 MB  conda-forge\n",
      "    brotli-1.0.9               |       h7f98852_5          17 KB  conda-forge\n",
      "    brotli-bin-1.0.9           |       h7f98852_5          19 KB  conda-forge\n",
      "    cudatoolkit-11.0.221       |       h6bb024c_0       953.0 MB  nvidia\n",
      "    cudf-21.06.01              |cuda_11.0_py37_g101fc0fda4_2       108.4 MB  rapidsai\n",
      "    cudnn-8.0.0                |       cuda11.0_0       403.2 MB  nvidia\n",
      "    cupy-8.0.0                 |   py37h0ce7dbb_0        42.2 MB  rapidsai\n",
      "    cytoolz-0.11.0             |   py37h5e8e339_3         403 KB  conda-forge\n",
      "    dask-2021.4.0              |     pyhd8ed1ab_0           4 KB  conda-forge\n",
      "    dask-core-2021.4.0         |     pyhd8ed1ab_0         718 KB  conda-forge\n",
      "    dask-cuda-21.08.00         |           py37_0         112 KB  rapidsai\n",
      "    dask-cudf-21.06.01         |py37_g101fc0fda4_2         103 KB  rapidsai\n",
      "    distributed-2021.4.0       |   py37h06a4308_0         1.0 MB\n",
      "    dlpack-0.5                 |       h9c3ff4c_0          12 KB  conda-forge\n",
      "    fastavro-1.4.4             |   py37h5e8e339_0         507 KB  conda-forge\n",
      "    fastrlock-0.6              |   py37hcd2ae1e_1          31 KB  conda-forge\n",
      "    heapdict-1.0.1             |             py_0           7 KB  conda-forge\n",
      "    libcudf-21.06.01           |cuda11.0_g101fc0fda4_2       187.7 MB  rapidsai\n",
      "    librmm-21.06.00            |cuda11.0_gee432a0_0          57 KB  rapidsai\n",
      "    locket-0.2.0               |             py_2           6 KB  conda-forge\n",
      "    msgpack-python-1.0.2       |   py37h2527ec5_1          91 KB  conda-forge\n",
      "    nccl-2.7.8.1               |     h4962215_100       144.5 MB  nvidia\n",
      "    nvtabular-0.5.3            |                0         219 KB  nvidia\n",
      "    nvtx-0.2.3                 |   py37h5e8e339_0          55 KB  conda-forge\n",
      "    pandas-1.2.5               |   py37h219a48f_0        11.8 MB  conda-forge\n",
      "    partd-1.2.0                |     pyhd8ed1ab_0          18 KB  conda-forge\n",
      "    psutil-5.8.0               |   py37h5e8e339_1         342 KB  conda-forge\n",
      "    pyarrow-1.0.1              |py37hb63ea2f_43_cuda         2.4 MB  conda-forge\n",
      "    pynvml-11.0.0              |     pyhd8ed1ab_0          39 KB  conda-forge\n",
      "    rmm-21.06.00               |cuda_11.0_py37_gee432a0_0         7.0 MB  rapidsai\n",
      "    sortedcontainers-2.4.0     |     pyhd8ed1ab_0          26 KB  conda-forge\n",
      "    spdlog-1.9.1               |       h4bd325d_0         341 KB  conda-forge\n",
      "    tblib-1.7.0                |     pyhd8ed1ab_0          15 KB  conda-forge\n",
      "    toolz-0.11.1               |             py_0          46 KB  conda-forge\n",
      "    zict-2.0.0                 |             py_0          10 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        1.85 GB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  arrow-cpp-proc     conda-forge/linux-64::arrow-cpp-proc-3.0.0-cuda\n",
      "  bokeh              conda-forge/linux-64::bokeh-2.3.3-py37h89c1867_0\n",
      "  brotli             conda-forge/linux-64::brotli-1.0.9-h7f98852_5\n",
      "  brotli-bin         conda-forge/linux-64::brotli-bin-1.0.9-h7f98852_5\n",
      "  cudatoolkit        nvidia/linux-64::cudatoolkit-11.0.221-h6bb024c_0\n",
      "  cudf               rapidsai/linux-64::cudf-21.06.01-cuda_11.0_py37_g101fc0fda4_2\n",
      "  cudnn              nvidia/linux-64::cudnn-8.0.0-cuda11.0_0\n",
      "  cupy               rapidsai/linux-64::cupy-8.0.0-py37h0ce7dbb_0\n",
      "  cytoolz            conda-forge/linux-64::cytoolz-0.11.0-py37h5e8e339_3\n",
      "  dask               conda-forge/noarch::dask-2021.4.0-pyhd8ed1ab_0\n",
      "  dask-core          conda-forge/noarch::dask-core-2021.4.0-pyhd8ed1ab_0\n",
      "  dask-cuda          rapidsai/linux-64::dask-cuda-21.08.00-py37_0\n",
      "  dask-cudf          rapidsai/linux-64::dask-cudf-21.06.01-py37_g101fc0fda4_2\n",
      "  distributed        pkgs/main/linux-64::distributed-2021.4.0-py37h06a4308_0\n",
      "  dlpack             conda-forge/linux-64::dlpack-0.5-h9c3ff4c_0\n",
      "  fastavro           conda-forge/linux-64::fastavro-1.4.4-py37h5e8e339_0\n",
      "  fastrlock          conda-forge/linux-64::fastrlock-0.6-py37hcd2ae1e_1\n",
      "  heapdict           conda-forge/noarch::heapdict-1.0.1-py_0\n",
      "  libcudf            rapidsai/linux-64::libcudf-21.06.01-cuda11.0_g101fc0fda4_2\n",
      "  librmm             rapidsai/linux-64::librmm-21.06.00-cuda11.0_gee432a0_0\n",
      "  locket             conda-forge/noarch::locket-0.2.0-py_2\n",
      "  msgpack-python     conda-forge/linux-64::msgpack-python-1.0.2-py37h2527ec5_1\n",
      "  nccl               nvidia/linux-64::nccl-2.7.8.1-h4962215_100\n",
      "  nvtabular          nvidia/noarch::nvtabular-0.5.3-0\n",
      "  nvtx               conda-forge/linux-64::nvtx-0.2.3-py37h5e8e339_0\n",
      "  partd              conda-forge/noarch::partd-1.2.0-pyhd8ed1ab_0\n",
      "  psutil             conda-forge/linux-64::psutil-5.8.0-py37h5e8e339_1\n",
      "  pynvml             conda-forge/noarch::pynvml-11.0.0-pyhd8ed1ab_0\n",
      "  rmm                rapidsai/linux-64::rmm-21.06.00-cuda_11.0_py37_gee432a0_0\n",
      "  sortedcontainers   conda-forge/noarch::sortedcontainers-2.4.0-pyhd8ed1ab_0\n",
      "  spdlog             conda-forge/linux-64::spdlog-1.9.1-h4bd325d_0\n",
      "  tblib              conda-forge/noarch::tblib-1.7.0-pyhd8ed1ab_0\n",
      "  toolz              conda-forge/noarch::toolz-0.11.1-py_0\n",
      "  zict               conda-forge/noarch::zict-2.0.0-py_0\n",
      "\n",
      "The following packages will be DOWNGRADED:\n",
      "\n",
      "  arrow-cpp                        5.0.0-py37h43e6a4c_0_cpu --> 1.0.1-py37h20531c0_43_cuda\n",
      "  pandas                               1.3.1-py37h219a48f_0 --> 1.2.5-py37h219a48f_0\n",
      "  pyarrow                          5.0.0-py37h58331f5_0_cpu --> 1.0.1-py37hb63ea2f_43_cuda\n",
      "\n",
      "\n",
      "Proceed ([y]/n)? \n",
      "\n",
      "Downloading and Extracting Packages\n",
      "nvtx-0.2.3           | 55 KB     | ########## | 100% \n",
      "cudnn-8.0.0          | 403.2 MB  | ########## | 100% \n",
      "brotli-1.0.9         | 17 KB     | ########## | 100% \n",
      "spdlog-1.9.1         | 341 KB    | ########## | 100% \n",
      "cytoolz-0.11.0       | 403 KB    | ########## | 100% \n",
      "dlpack-0.5           | 12 KB     | ########## | 100% \n",
      "zict-2.0.0           | 10 KB     | ########## | 100% \n",
      "nvtabular-0.5.3      | 219 KB    | ########## | 100% \n",
      "arrow-cpp-1.0.1      | 21.0 MB   | ########## | 100% \n",
      "pandas-1.2.5         | 11.8 MB   | ########## | 100% \n",
      "heapdict-1.0.1       | 7 KB      | ########## | 100% \n",
      "dask-core-2021.4.0   | 718 KB    | ########## | 100% \n",
      "pyarrow-1.0.1        | 2.4 MB    | ########## | 100% \n",
      "dask-cudf-21.06.01   | 103 KB    | ########## | 100% \n",
      "libcudf-21.06.01     | 187.7 MB  | ########## | 100% \n",
      "nccl-2.7.8.1         | 144.5 MB  | ########## | 100% \n",
      "toolz-0.11.1         | 46 KB     | ########## | 100% \n",
      "msgpack-python-1.0.2 | 91 KB     | ########## | 100% \n",
      "partd-1.2.0          | 18 KB     | ########## | 100% \n",
      "cupy-8.0.0           | 42.2 MB   | ########## | 100% \n",
      "librmm-21.06.00      | 57 KB     | ########## | 100% \n",
      "locket-0.2.0         | 6 KB      | ########## | 100% \n",
      "fastrlock-0.6        | 31 KB     | ########## | 100% \n",
      "cudatoolkit-11.0.221 | 953.0 MB  | ########## | 100% \n",
      "dask-cuda-21.08.00   | 112 KB    | ########## | 100% \n",
      "brotli-bin-1.0.9     | 19 KB     | ########## | 100% \n",
      "rmm-21.06.00         | 7.0 MB    | ########## | 100% \n",
      "sortedcontainers-2.4 | 26 KB     | ########## | 100% \n",
      "psutil-5.8.0         | 342 KB    | ########## | 100% \n",
      "distributed-2021.4.0 | 1.0 MB    | ########## | 100% \n",
      "cudf-21.06.01        | 108.4 MB  | ########## | 100% \n",
      "fastavro-1.4.4       | 507 KB    | ########## | 100% \n",
      "arrow-cpp-proc-3.0.0 | 24 KB     | ########## | 100% \n",
      "dask-2021.4.0        | 4 KB      | ########## | 100% \n",
      "tblib-1.7.0          | 15 KB     | ########## | 100% \n",
      "pynvml-11.0.0        | 39 KB     | ########## | 100% \n",
      "bokeh-2.3.3          | 8.2 MB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n",
      "\n",
      "done\n",
      "Removing intermediate container 53c998e17774\n",
      " ---> c2675f8f44a5\n",
      "Step 4/5 : ENV LD_LIBRARY_PATH /usr/local/cuda/lib:/usr/local/cuda/lib64:/usr/local/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      " ---> Running in 420d0857c66b\n",
      "Removing intermediate container 420d0857c66b\n",
      " ---> 13c3dfab973f\n",
      "Step 5/5 : COPY train.py ./\n",
      " ---> 2bb11c599a0c\n",
      "Successfully built 2bb11c599a0c\n",
      "Successfully tagged gcr.io/jk-mlops-dev/hugectr_train:latest\n"
     ]
    }
   ],
   "source": [
    "TRAIN_IMAGE = f'gcr.io/{PROJECT}/hugectr_train'\n",
    "\n",
    "!docker build -t {TRAIN_IMAGE} {SCRIPT_FOLDER}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad496a92",
   "metadata": {},
   "source": [
    "## Prepare and submit a job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71dddf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=GCS_BUCKET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7e6029",
   "metadata": {},
   "source": [
    "### Prepare worker pool specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c80c9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "CRITEO_PARQUET = 'gs://jk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce5c5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_pool_specs =  [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\",\n",
    "            \"accelerator_type\": \"NVIDIA_TESLA_T4\",\n",
    "            \"accelerator_count\": 1,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_IMAGE,\n",
    "            \"command\": [\"python\", \"train.py\"],\n",
    "            \"args\": [\n",
    "                '--input_train=' + , \n",
    "                '--per_replica_batch_size=128',\n",
    "                '--training_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}',\n",
    "                '--validation_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_VALID_SPLIT_NAME}',\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m76",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m76"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
